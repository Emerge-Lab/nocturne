{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "from utils.config import load_config_nb\n",
    "\n",
    "# Configuration for plotting and warnings\n",
    "sns.set(context='notebook', font_scale=1.1, \n",
    "        style='ticks', rc={'figure.figsize': (8, 3), 'figure.facecolor': 'none', 'axes.facecolor': 'none'})\n",
    "plt.set_loglevel('WARNING')\n",
    "plt.rcParams.update({'lines.markeredgewidth': 1})\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trained models\n",
    "models_config = load_config_nb('model_config') \n",
    "\n",
    "# Environment configurations\n",
    "env_config = load_config_nb('env_config')\n",
    "\n",
    "MAPPING = {\n",
    "    np.nan: \"BC\",\n",
    "    0.0: \"PPO\",\n",
    "    0.02: \"HR-PPO (λ = 0.02)\",\n",
    "    0.06: \"HR-PPO (λ = 0.06)\",\n",
    "    0.07: \"HR-PPO (λ = 0.07)\",\n",
    "    0.08: \"HR-PPO (λ = 0.08)\",\n",
    "}\n",
    "\n",
    "AGENT_MAP = {\n",
    "    'MA HR-PPO': 'HR-PPO',\n",
    "}\n",
    "\n",
    "def std_error(ser, scale=100):\n",
    "    \"\"\"\n",
    "    Calculate the standard error for aggregation in a pivot table in percentages (scale=100).\n",
    "\n",
    "    Parameters:\n",
    "        x (array-like): Array-like object containing the values of the column being aggregated for each group.\n",
    "\n",
    "    Returns:\n",
    "        float: Standard error of the values in the group.\n",
    "\n",
    "    Notes:\n",
    "        Standard error is calculated as the standard deviation divided by the square root of the number of values in the group.\n",
    "    \"\"\"\n",
    "    return (np.std(ser, ddof=1) / np.sqrt(len(ser))) * scale\n",
    "\n",
    "def calculate_scene_rate(df, metric):\n",
    "    df[f'{metric}_scene'] = df.groupby('scene_id')[metric].transform('sum') / df.groupby('scene_id')['scene_id'].transform('count')\n",
    "\n",
    "def mean_perc(ser: pd.Series) -> float:\n",
    "    return ser.mean() * 100    \n",
    "\n",
    "INT_BINS = [-1, 0, 1, 2, float('inf')]\n",
    "INT_LABELS = ['0', '1', '2', '3+']\n",
    "\n",
    "STEP_BINS = [0, 10, 20, 30, 40, float('inf')]\n",
    "STEP_LABELS = ['0-1 s', '1-2 s', '2-3 s', '3-4 s', '4+ s']\n",
    "\n",
    "\n",
    "TOT_INT_BINS = [-1, 2, 4, 6, 8, float('inf')]\n",
    "TOT_INT_LABELS = ['0', '2', '4',  '6', '8+']\n",
    "\n",
    "# Path to save videos\n",
    "VIDEO_PATH = \"../evaluation/videos\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe with the results\n",
    "path = '../evaluation/results/df_paper_agg_performance.csv'\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "df['Reg. weight'] = df['Reg. weight'].replace(MAPPING)\n",
    "df['Agent'] = df[\"Reg. weight\"].replace(AGENT_MAP)\n",
    "df['int_path_cat'] = pd.cut(df['veh_int_paths'], bins=INT_BINS, labels=INT_LABELS, include_lowest=True)\n",
    "df['step_diff_cat'] = pd.cut(df['min_step_diff'], bins=STEP_BINS, labels=STEP_LABELS, include_lowest=True)\n",
    "df['int_path_cat_tot'] = pd.cut(df['tot_int_paths'], bins=TOT_INT_BINS, labels=TOT_INT_LABELS, include_lowest=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_eval_mode_bar(df, save=True, fig_name='self_play_log_replay_comp', \n",
    "                       x_axis_order=['Self-play', 'Log-replay'], \n",
    "                       colors=['b', 'darkorange', 'tab:purple'], alpha=0.5\n",
    "                    ):\n",
    "    \"\"\"Plot the overall effectiveness of agents across evaluation modes.\"\"\"\n",
    "    \n",
    "    #df['Agent'].replace(to_replace=r'HR-PPO \\| λ = [\\d.]+', value='HR-PPO', regex=True, inplace=True)\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 3, figsize=(11, 3))\n",
    "\n",
    "    sns.barplot(data=df, x='Eval mode', y='veh_veh_collision', hue='Agent', errorbar='sd', palette=colors, order=x_axis_order, ax=axs[1], legend=False);\n",
    "    axs[1].grid(True, alpha=alpha)\n",
    "    axs[1].set_xlabel('Evaluation mode', labelpad=10, fontsize=10)\n",
    "    axs[1].set_ylabel('Collision rate [%]')\n",
    "\n",
    "    sns.barplot(data=df, x='Eval mode', y='goal_rate', hue='Agent', errorbar='sd', palette=colors, order=x_axis_order, ax=axs[0], legend=False);\n",
    "    axs[0].grid(True, alpha=alpha)\n",
    "    axs[0].set_xlabel('Evaluation mode', labelpad=10, fontsize=10)\n",
    "    axs[0].set_ylabel('Goal rate [%]')\n",
    "\n",
    "    sns.barplot(data=df, x='Eval mode', y='off_road', hue='Agent', errorbar='sd', palette=colors, order=x_axis_order, ax=axs[2], legend=True);\n",
    "    axs[2].grid(True, alpha=alpha)\n",
    "    axs[2].set_xlabel('Evaluation mode', labelpad=10, fontsize=10)\n",
    "    axs[2].set_ylabel('Off-Road [%]')\n",
    "    \n",
    "    axs[0].set_ylim([0, 100])\n",
    "\n",
    "    axs[2].legend(loc='upper left', title='Agent', fontsize=12, bbox_to_anchor=(1, 1))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    sns.despine()\n",
    "    \n",
    "    if save:\n",
    "        plt.savefig(f'../evaluation/figures/coordination/{fig_name}.pdf', bbox_inches=\"tight\")\n",
    "        \n",
    "        \n",
    "def plot_collision_interactivity(\n",
    "    df_self_play, \n",
    "    df_log_replay, \n",
    "    coll_diffs, \n",
    "    x_metric, \n",
    "    save=True, \n",
    "    fig_name='collision_interactivity', \n",
    "    colors=['tab:purple', 'darkorange', 'b']\n",
    "    ):\n",
    "    \"\"\"Plot the collision rate as a function of the intersecting paths.\"\"\"\n",
    "    \n",
    "    coll_diffs['Agent'].replace(to_replace=r'HR-PPO \\| λ = [\\d.]+', value='HR-PPO', regex=True, inplace=True)   \n",
    "    \n",
    "    # Multiply the 'veh_veh_collision' values by 100\n",
    "    df_self_play['veh_veh_collision'] *= 100\n",
    "    df_log_replay['veh_veh_collision'] *= 100\n",
    "    coll_diffs['diffs'] *= 100\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 3, figsize=(12, 4), sharey=False, sharex=False)\n",
    "\n",
    "    sns.lineplot(\n",
    "        x=x_metric, \n",
    "        y='veh_veh_collision', \n",
    "        data=df_self_play, \n",
    "        palette=colors,\n",
    "        hue='Agent',\n",
    "        err_style='bars',  \n",
    "        linewidth=2,\n",
    "        markers=True, \n",
    "        errorbar='se',   \n",
    "        ax=axs[0],\n",
    "        legend=False\n",
    "    )\n",
    "    axs[0].set_title('Self-play')\n",
    "    axs[0].set_ylim([0, 70])\n",
    "    axs[0].grid(True, alpha=0.5)\n",
    "    axs[0].set_xlabel('Number of intersecting paths', labelpad=10)\n",
    "    axs[0].set_ylabel('Collision rate [%]')\n",
    "\n",
    "    sns.lineplot(\n",
    "        x=x_metric, \n",
    "        y='veh_veh_collision', \n",
    "        data=df_log_replay, \n",
    "        hue='Agent',\n",
    "        palette=colors,\n",
    "        err_style='bars',  \n",
    "        linewidth=2,\n",
    "        markers=True, \n",
    "        errorbar='se',    \n",
    "        ax=axs[1],\n",
    "        legend=False,\n",
    "    );\n",
    "\n",
    "    axs[1].set_title('Log-replay')\n",
    "    axs[1].grid(True, alpha=0.5)\n",
    "    axs[1].set_ylim([0, 70])\n",
    "    axs[1].set_xlabel('Number of intersecting paths', labelpad=10)\n",
    "    axs[1].set_ylabel('Collision rate [%]')\n",
    "    \n",
    "    sns.barplot(\n",
    "        x=x_metric, \n",
    "        y='diffs', \n",
    "        data=coll_diffs, \n",
    "        hue='Agent',\n",
    "        hue_order=['PPO', 'HR-PPO (λ = 0.07)', 'BC'],\n",
    "        palette=colors,\n",
    "        linewidth=2,\n",
    "        ax=axs[2],\n",
    "        legend=False,\n",
    "    );\n",
    "\n",
    "    axs[2].set_title('Increase in collision rate \\n when switching to log-replay')\n",
    "    axs[2].grid(True, alpha=0.5)\n",
    "    axs[2].set_ylim([0, 70])\n",
    "    axs[2].set_xlabel('Number of intersecting paths', labelpad=10)\n",
    "    axs[2].set_ylabel('∆ Collision rate [%]')\n",
    "    axs[0].legend(loc='upper left', title='Agent', labels=['PPO', 'HR-PPO', 'BC'], facecolor='white', fontsize=12)#, bbox_to_anchor=, 1))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    sns.despine()\n",
    "    \n",
    "    if save:\n",
    "        plt.savefig(f'../evaluation/figures/coordination/{fig_name}.pdf', bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Overall performance** self-play vs. log-replay across datasets\n",
    "\n",
    "- Select best HR-PPO agent\n",
    "- Select best PPO agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the scene-level aggregates to calculate the standard errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_select_top = df[df['Reg. weight'].isin(['BC', 'PPO', 'HR-PPO (λ = 0.06)'])]\n",
    "\n",
    "df_bc = df[df['Reg. weight'] == 'BC']\n",
    "df_ppo = df[df['Reg. weight'] == 'PPO']\n",
    "df_hr_ppo = df[df['Reg. weight'] == 'HR-PPO (λ = 0.06)']\n",
    "\n",
    "\n",
    "# Calculate the scene rates\n",
    "calculate_scene_rate(df_bc, 'goal_rate')\n",
    "calculate_scene_rate(df_bc, 'off_road')\n",
    "calculate_scene_rate(df_bc, 'veh_veh_collision')\n",
    "\n",
    "calculate_scene_rate(df_ppo, 'goal_rate')\n",
    "calculate_scene_rate(df_ppo, 'off_road')\n",
    "calculate_scene_rate(df_ppo, 'veh_veh_collision')\n",
    "\n",
    "calculate_scene_rate(df_hr_ppo, 'goal_rate')\n",
    "calculate_scene_rate(df_hr_ppo, 'off_road')\n",
    "calculate_scene_rate(df_hr_ppo, 'veh_veh_collision')\n",
    "\n",
    "# Concate the dataframes\n",
    "df_concat = pd.concat([df_bc, df_ppo, df_hr_ppo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the scene level stats to compute the standard error\n",
    "df_agg_perf_scene_level = df_concat.pivot_table(\n",
    "    index=['Agent', 'Train agent', 'Dataset', 'Eval mode'],\n",
    "    values=['goal_rate_scene', 'veh_veh_collision_scene', 'off_road_scene'],\n",
    "    aggfunc=std_error, # Use for table: mean_plus_binary_se\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_agg_perf_scene_level) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print((df_agg_perf_scene_level.style.format(precision=2)).to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Overall performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_perf = df_select_top.pivot_table(\n",
    "    index=['Agent', 'Train agent', 'Dataset', 'Eval mode'],\n",
    "    values=['goal_rate', 'veh_veh_collision', 'off_road'],\n",
    "    aggfunc=mean_perc, # Use for table: mean_plus_binary_se\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_agg_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print((df_agg_perf.style.format(precision=2)).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_perf = df_agg_perf.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_perf_train = df_agg_perf[(df_agg_perf['Dataset'] == 'Train') & ((df_agg_perf['Train agent'] != 'Log-replay'))]\n",
    "\n",
    "plot_eval_mode_bar(df_agg_perf_train, save=True, fig_name='train_eval_mode_bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_perf_test = df_agg_perf[(df_agg_perf['Dataset'] == 'Test') & ((df_agg_perf['Train agent'] != 'Log-replay'))]\n",
    "\n",
    "plot_eval_mode_bar(df_agg_perf_test, save=True, fig_name='test_eval_mode_bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Interactivity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC = 'int_path_cat'\n",
    "\n",
    "df_select_top = df[df['Agent'].isin(['PPO', 'BC', 'HR-PPO (λ = 0.07)'])]\n",
    "df_select_top = df_select_top[df_select_top['Dataset'] == 'Test']\n",
    "df_select_top['Agent'].replace(to_replace='HR-PPO (λ = 0.07)', value='HR-PPO', regex=True, inplace=True)   \n",
    "\n",
    "df_select_top_sp = df_select_top[df_select_top['Eval mode'] == 'Self-play']\n",
    "df_select_top_lr = df_select_top[df_select_top['Eval mode'] == 'Log-replay']\n",
    "\n",
    "int_path_sp = df_select_top_sp.groupby(['Agent', METRIC]).agg({'veh_veh_collision': 'mean'}).reset_index()\n",
    "coll_diffs = df_select_top_lr.groupby(['Agent', METRIC]).agg({'veh_veh_collision': 'mean'}).reset_index()\n",
    "\n",
    "coll_diffs['diffs'] = coll_diffs['veh_veh_collision'] - int_path_sp['veh_veh_collision']\n",
    "\n",
    "plot_collision_interactivity(df_select_top_sp, df_select_top_lr, coll_diffs, METRIC, save=True, fig_name='collision_interactivity_test_10k')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nocturne_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
