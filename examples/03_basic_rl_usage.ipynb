{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic RL usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing environments\n",
    "\n",
    "\n",
    "#### **Environment settings**\n",
    "\n",
    "- Initializing an environment is done with the `BaseEnv` class. The `BaseEnv` class leverages the `nocturne` simulator to create a basic RL interface, based on the provided traffic scenario(s). \n",
    "\n",
    "---\n",
    "> üìù The `env_config.yaml` file defines our environment settings, such as the action space, observation space and traffic scenarios to use.\n",
    "---\n",
    "\n",
    "Check out `configs/env_config` for all the details!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from nocturne.envs.base_env import BaseEnv\n",
    "\n",
    "# Load environment settings\n",
    "with open(f\"../configs/env_config.yaml\", \"r\") as stream:\n",
    "    env_config = yaml.safe_load(stream)\n",
    "\n",
    "# Initialize environment\n",
    "env = BaseEnv(\n",
    "    config=env_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "controlling agents # [32, 3]\n"
     ]
    }
   ],
   "source": [
    "print(f'controlling agents # {[agent.id for agent in env.controlled_vehicles]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data**\n",
    "\n",
    "- Within `env_config.yaml`, we specify the path to the folder containing the traffic scenarios to use as follows:\n",
    "\n",
    "```yaml\n",
    "# Path to folder with traffic scene(s) from which to create an environment\n",
    "data_path: ../data\n",
    "```\n",
    "\n",
    "- [Here](https://github.com/facebookresearch/nocturne/tree/main#downloading-the-dataset) are the instructions to access the complete dataset of traffic scenes. \n",
    "\n",
    "- The data folder also has a file named `valid_files.json`. This file lists the names of all the valid traffic scenarios along with the ids of the vehicles that are not valid. These vehicles are excluded from our experiment.\n",
    "\n",
    "For simplicity, we currently added a single traffic scenario that includes two vehicles in our data folder. Both vehicles can be used, so our `valid_files.json` looks like this:\n",
    "\n",
    "```yaml\n",
    "{\n",
    "    \"example_scenario.json\": []\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interacting with the environment\n",
    "\n",
    "The classic agent-environment loop of reinforcement learning is implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done after 80 steps -- total return in episode: {3: 1.8345003977789347, 32: 2.3487226973552358}\n",
      "Done after 80 steps -- total return in episode: {3: 0.43543555449700166, 32: 1.549664751141132}\n",
      "Done after 80 steps -- total return in episode: {3: -0.223526138403951, 32: 2.0764939767882815}\n",
      "Done after 37 steps -- total return in episode: {3: 8.888294189938877, 32: 0.9171524851397167}\n",
      "Done after 80 steps -- total return in episode: {3: 0.3975525642124312, 32: 2.1566675033281726}\n",
      "Done after 80 steps -- total return in episode: {3: 1.6119904903044686, 32: 0.9667098631259777}\n",
      "Done after 80 steps -- total return in episode: {3: 0.06271202021346704, 32: 2.0584825171460746}\n",
      "Done after 80 steps -- total return in episode: {3: 1.3140901879973366, 32: 1.2298469684309603}\n",
      "Done after 80 steps -- total return in episode: {3: 1.530614215964841, 32: 1.8252810539055202}\n",
      "Done after 80 steps -- total return in episode: {3: 1.2409864172219975, 32: 1.379805712013483}\n",
      "Done after 80 steps -- total return in episode: {3: 0.47753240450321816, 32: 2.280355875618669}\n",
      "Done after 80 steps -- total return in episode: {3: 0.6084321033989238, 32: 1.862401833940641}\n",
      "Done after 80 steps -- total return in episode: {3: 1.5653458434477148, 32: 2.1426790749750992}\n"
     ]
    }
   ],
   "source": [
    "# Reset\n",
    "obs_dict = env.reset()\n",
    "\n",
    "# Get info\n",
    "agent_ids = [agent_id for agent_id in obs_dict.keys()]\n",
    "dead_agent_ids = []\n",
    "num_agents = len(agent_ids)\n",
    "rewards = {agent_id: 0 for agent_id in agent_ids}\n",
    "\n",
    "for step in range(1000):\n",
    "\n",
    "    # Sample actions\n",
    "    action_dict = {\n",
    "        agent_id: env.action_space.sample() \n",
    "        for agent_id in agent_ids\n",
    "        if agent_id not in dead_agent_ids\n",
    "    }\n",
    "    \n",
    "    # Step in env\n",
    "    obs_dict, rew_dict, done_dict, info_dict = env.step(action_dict)\n",
    "\n",
    "    for agent_id in action_dict.keys():\n",
    "        rewards[agent_id] += rew_dict[agent_id]\n",
    "\n",
    "    # Update dead agents\n",
    "    for agent_id, is_done in done_dict.items():\n",
    "        if is_done and agent_id not in dead_agent_ids:\n",
    "            dead_agent_ids.append(agent_id)\n",
    "\n",
    "    # Reset if all agents are done\n",
    "    if done_dict[\"__all__\"]:\n",
    "        print(f'Done after {env.step_num} steps -- total return in episode: {rewards}')\n",
    "        obs_dict = env.reset()\n",
    "        dead_agent_ids = []\n",
    "        rewards = {agent_id: 0 for agent_id in agent_ids}\n",
    "\n",
    "# Close environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing information about the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-inf, inf, (10,), float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The observation space \n",
    "env.observation_space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The size of the joint action space \n",
    "env.action_space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<nocturne_cpp.Vehicle at 0x13579ccf0>, <nocturne_cpp.Vehicle at 0x13579ebb0>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which agents are controlled?\n",
    "env.controlled_vehicles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
