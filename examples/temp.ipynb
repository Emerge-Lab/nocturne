{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15, 10])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gymnasium.spaces import MultiDiscrete\n",
    "import numpy as np\n",
    "action_space = MultiDiscrete(np.array([20,25]), seed=42)\n",
    "action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17, 17])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nocturne.envs.nocturne_gymnasium import NocturneGymnasium\n",
    "import yaml\n",
    "from nocturne.envs.base_env import BaseEnv\n",
    "from nocturne.envs.vec_env_ma import MultiAgentAsVecEnv\n",
    "\n",
    "# Load environment settings\n",
    "with open(f\"../configs/env_config.yaml\", \"r\") as stream:\n",
    "    env_config = yaml.safe_load(stream)\n",
    "\n",
    "# Initialize environment\n",
    "env = BaseEnv(config=env_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gymnasiumEnv = NocturneGymnasium(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiDiscrete([20 25])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gymnasiumEnv.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{6: array([0.30362597, 0.54050583, 0.16309013, ..., 0.        , 0.        ,\n",
       "        0.        ]),\n",
       " 23: array([0.33037499, 0.583     , 0.15996636, ..., 0.        , 0.        ,\n",
       "        0.        ]),\n",
       " 2: array([0.29274434, 0.50419724, 0.15200901, ..., 0.        , 0.        ,\n",
       "        0.        ]),\n",
       " 8: array([0.30145869, 0.53301573, 0.16035738, ..., 0.        , 0.        ,\n",
       "        0.        ]),\n",
       " 9: array([0.27850392, 0.50491506, 0.16618462, ..., 0.        , 0.        ,\n",
       "        0.        ]),\n",
       " 1: array([0.28742164, 0.52419186, 0.17056067, ..., 0.        , 0.        ,\n",
       "        0.        ]),\n",
       " 14: array([0.27822894, 0.51315653, 0.1849147 , ..., 0.        , 0.        ,\n",
       "        0.        ]),\n",
       " 18: array([0.28400436, 0.51011646, 0.13745898, ..., 0.        , 0.        ,\n",
       "        0.        ]),\n",
       " 4: array([0.31240156, 0.52112043, 0.16890202, ..., 0.        , 0.        ,\n",
       "        0.        ]),\n",
       " 0: array([0.27941427, 0.51273805, 0.16114239, ..., 0.        , 0.        ,\n",
       "        0.        ])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gymnasiumEnv.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset\n",
    "obs_dict = gymnasiumEnv.reset()\n",
    "\n",
    "# Get info\n",
    "agent_ids = [agent_id for agent_id in obs_dict.keys()]\n",
    "dead_agent_ids = []\n",
    "num_agents = len(agent_ids)\n",
    "rewards = {agent_id: 0 for agent_id in agent_ids}\n",
    "\n",
    "for step in range(1000):\n",
    "\n",
    "    # Sample actions\n",
    "    action_dict = {\n",
    "        agent_id: env.action_space.sample() \n",
    "        for agent_id in agent_ids\n",
    "        if agent_id not in dead_agent_ids\n",
    "    }\n",
    "    # Step in env\n",
    "    obs_dict, rew_dict, done_dict, info_dict = gymnasiumEnv.step(action_dict)\n",
    "\n",
    "    for agent_id in action_dict.keys():\n",
    "        rewards[agent_id] += rew_dict[agent_id]\n",
    "\n",
    "    # Update dead agents\n",
    "    for agent_id, is_done in done_dict.items():\n",
    "        if is_done and agent_id not in dead_agent_ids:\n",
    "            dead_agent_ids.append(agent_id)\n",
    "\n",
    "    # Reset if all agents are done\n",
    "    if done_dict[\"__all__\"]:\n",
    "        print(f'Done after {env.step_num} steps -- total return in episode: {rewards}')\n",
    "        obs_dict = gymnasiumEnv.reset()\n",
    "        agent_ids = [agent_id for agent_id in obs_dict.keys()]\n",
    "        dead_agent_ids = []\n",
    "        rewards = {agent_id: 0 for agent_id in agent_ids}\n",
    "\n",
    "# Close environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import SubprocVecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_config):\n",
    "    return NocturneGymnasium(BaseEnv(config=env_config)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = SubprocVecEnv([lambda: make_env(env_config) for _ in range(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset\n",
    "obs_dicts = envs.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_ids_batch = []\n",
    "dead_agent_ids_batch = []\n",
    "num_agents_batch = []\n",
    "rewards_batch = []\n",
    "for obs_dict in obs_dicts:\n",
    "    agent_ids = [agent_id for agent_id in obs_dict.keys()]\n",
    "    dead_agent_ids = []\n",
    "    num_agents = len(agent_ids)\n",
    "    rewards = {agent_id: 0 for agent_id in agent_ids}\n",
    "    agent_ids_batch.append(agent_ids)\n",
    "    dead_agent_ids_batch.append(dead_agent_ids)\n",
    "    num_agents_batch.append(num_agents)\n",
    "    rewards_batch.append(rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{6: 6, 23: 18, 2: 24, 8: 1, 9: 2, 1: 18, 14: 1, 18: 13, 4: 11, 0: 18},\n",
       " {6: 0, 23: 14, 2: 8, 8: 22, 9: 1, 1: 4, 14: 18, 18: 18, 4: 1, 0: 23},\n",
       " {6: 9, 23: 22, 2: 1, 8: 2, 9: 0, 1: 20, 14: 5, 18: 14, 4: 7, 0: 13},\n",
       " {6: 13, 23: 1, 2: 8, 8: 13, 9: 9, 1: 16, 14: 9, 18: 9, 4: 12, 0: 23}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_dicts = [\n",
    "        {\n",
    "            agent_id: env.action_space.sample() \n",
    "            for agent_id in agent_ids\n",
    "            if agent_id not in dead_agent_ids\n",
    "        }\n",
    "        for agent_ids, dead_agent_ids in zip(agent_ids_batch, dead_agent_ids_batch)\n",
    "    ]\n",
    "action_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "7",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rew_dict, rewards \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(rew_dicts, rewards_batch):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m agent_id \u001b[38;5;129;01min\u001b[39;00m rew_dict\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m---> 17\u001b[0m         \u001b[43mrewards\u001b[49m\u001b[43m[\u001b[49m\u001b[43magent_id\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rew_dict[agent_id] \n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Update dead agents\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m done_dict, dead_agent_ids \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(done_dicts, dead_agent_ids_batch):\n",
      "\u001b[0;31mKeyError\u001b[0m: 7"
     ]
    }
   ],
   "source": [
    "for step in range(1000):\n",
    "\n",
    "    # Sample actions\n",
    "    action_dicts = [\n",
    "        {\n",
    "            agent_id: env.action_space.sample() \n",
    "            for agent_id in agent_ids\n",
    "            if agent_id not in dead_agent_ids\n",
    "        }\n",
    "        for agent_ids, dead_agent_ids in zip(agent_ids_batch, dead_agent_ids_batch)\n",
    "    ]\n",
    "    # Step in env\n",
    "    obs_dicts, rew_dicts, done_dicts, info_dicts = envs.step(action_dicts)\n",
    "\n",
    "    for rew_dict, rewards in zip(rew_dicts, rewards_batch):\n",
    "        for agent_id in rew_dict.keys():\n",
    "            rewards[agent_id] += rew_dict[agent_id] \n",
    "    \n",
    "    # Update dead agents\n",
    "    for done_dict, dead_agent_ids in zip(done_dicts, dead_agent_ids_batch):\n",
    "        for agent_id, is_done in done_dict.items():\n",
    "            if is_done and agent_id not in dead_agent_ids:\n",
    "                dead_agent_ids.append(agent_id)\n",
    "\n",
    "    # Reset if all agents are done\n",
    "    if all([done_dict[\"__all__\"] for done_dict in done_dicts]):\n",
    "        print(f'Done after {env.step_num} steps -- total return in episode: {rewards}')\n",
    "        obs_dicts = envs.reset()\n",
    "        agent_ids_batch = []\n",
    "        dead_agent_ids_batch = []\n",
    "        num_agents_batch = []\n",
    "        rewards_batch = []\n",
    "        for obs_dict in obs_dicts:\n",
    "            agent_ids = [agent_id for agent_id in obs_dict.keys()]\n",
    "            dead_agent_ids = []\n",
    "            num_agents = len(agent_ids)\n",
    "            rewards = {agent_id: 0 for agent_id in agent_ids}\n",
    "            agent_ids_batch.append(agent_ids)\n",
    "            dead_agent_ids_batch.append(dead_agent_ids)\n",
    "            num_agents_batch.append(num_agents)\n",
    "            rewards_batch.append(rewards)\n",
    "\n",
    "    # # Sample actions\n",
    "    # action_dict = {\n",
    "    #     agent_id: env.action_space.sample() \n",
    "    #     for agent_id in agent_ids\n",
    "    #     if agent_id not in dead_agent_ids\n",
    "    # }\n",
    "    # # Step in env\n",
    "    # obs_dict, rew_dict, done_dict, info_dict = gymnasiumEnv.step(action_dict)\n",
    "\n",
    "    # for agent_id in action_dict.keys():\n",
    "    #     rewards[agent_id] += rew_dict[agent_id]\n",
    "\n",
    "    # # Update dead agents\n",
    "    # for agent_id, is_done in done_dict.items():\n",
    "    #     if is_done and agent_id not in dead_agent_ids:\n",
    "    #         dead_agent_ids.append(agent_id)\n",
    "\n",
    "    # # Reset if all agents are done\n",
    "    # if done_dict[\"__all__\"]:\n",
    "    #     print(f'Done after {env.step_num} steps -- total return in episode: {rewards}')\n",
    "    #     obs_dict = gymnasiumEnv.reset()\n",
    "    #     agent_ids = [agent_id for agent_id in obs_dict.keys()]\n",
    "    #     dead_agent_ids = []\n",
    "    #     rewards = {agent_id: 0 for agent_id in agent_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
