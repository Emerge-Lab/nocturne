{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cast a multi-agent env as vec env to use SB3's PPO.\"\"\"\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import List, Union\n",
    "import typer\n",
    "import torch\n",
    "import wandb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Multi-agent as vectorized environment\n",
    "from nocturne.envs.vec_env_ma import MultiAgentAsVecEnv\n",
    "from utils.config import load_config\n",
    "from utils.render import make_video\n",
    "from box import Box\n",
    "# Custom callback\n",
    "from utils.sb3.callbacks import CustomMultiAgentCallback\n",
    "import yaml\n",
    "# Custom PPO class that supports multi-agent control\n",
    "from utils.sb3.custom_ppo import MultiAgentPPO\n",
    "from utils.string_utils import datetime_to_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"/home/emerge/daphne/nocturne_lab/configs/env_config.yaml\", \"r\") as stream:\n",
    "    env_config = Box(yaml.safe_load(stream))\n",
    "env_config.data_path = \"/home/emerge/daphne/nocturne_lab/data_tmp/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MultiAgentAsVecEnv(\n",
    "    config=env_config, \n",
    "    num_envs=env_config.max_num_vehicles,\n",
    ")\n",
    "\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 6730)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6730,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs[0, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.28085133 0.49425942 0.00275741 ... 0.         0.         0.        ]\n",
      " [0.33037499 0.583      0.08777953 ... 0.         0.         0.        ]\n",
      " [       nan        nan        nan ...        nan        nan        nan]\n",
      " ...\n",
      " [       nan        nan        nan ...        nan        nan        nan]\n",
      " [       nan        nan        nan ...        nan        nan        nan]\n",
      " [       nan        nan        nan ...        nan        nan        nan]]\n"
     ]
    }
   ],
   "source": [
    "# The ego state\n",
    "obs[:10, :].shape\n",
    "\n",
    "print(obs[:10, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "This is the base MLP in stable baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1068 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 3    |\n",
      "|    total_timesteps | 3747 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 956         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 8049        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011197504 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.21       |\n",
      "|    explained_variance   | 0.00433     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.133       |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    value_loss           | 0.552       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 912        |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 13         |\n",
      "|    total_timesteps      | 12327      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01326931 |\n",
      "|    clip_fraction        | 0.138      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -3.19      |\n",
      "|    explained_variance   | 0.48       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0907     |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.00976   |\n",
      "|    value_loss           | 0.295      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 884        |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 18         |\n",
      "|    total_timesteps      | 16249      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01013521 |\n",
      "|    clip_fraction        | 0.113      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -3.17      |\n",
      "|    explained_variance   | 0.618      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0676     |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.00929   |\n",
      "|    value_loss           | 0.392      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 878         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 20435       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011490653 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.16       |\n",
      "|    explained_variance   | 0.756       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0445      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00719    |\n",
      "|    value_loss           | 0.552       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 870         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 28          |\n",
      "|    total_timesteps      | 24414       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009480001 |\n",
      "|    clip_fraction        | 0.0982      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.17       |\n",
      "|    explained_variance   | 0.824       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.186       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00612    |\n",
      "|    value_loss           | 0.493       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 870         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 32          |\n",
      "|    total_timesteps      | 28448       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010709957 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.17       |\n",
      "|    explained_variance   | 0.813       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0179      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0126     |\n",
      "|    value_loss           | 0.3         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 869         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 37          |\n",
      "|    total_timesteps      | 32487       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010682907 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.16       |\n",
      "|    explained_variance   | 0.832       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00763     |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00892    |\n",
      "|    value_loss           | 0.266       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 867         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 41          |\n",
      "|    total_timesteps      | 36411       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011045458 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.16       |\n",
      "|    explained_variance   | 0.698       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0798      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00707    |\n",
      "|    value_loss           | 0.877       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 866        |\n",
      "|    iterations           | 10         |\n",
      "|    time_elapsed         | 46         |\n",
      "|    total_timesteps      | 40468      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01576655 |\n",
      "|    clip_fraction        | 0.166      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -3.14      |\n",
      "|    explained_variance   | 0.843      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0286     |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.0123    |\n",
      "|    value_loss           | 0.4        |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 866          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 51           |\n",
      "|    total_timesteps      | 44637        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0116510205 |\n",
      "|    clip_fraction        | 0.137        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.13        |\n",
      "|    explained_variance   | 0.881        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.054        |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.0118      |\n",
      "|    value_loss           | 0.355        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 866         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 56          |\n",
      "|    total_timesteps      | 48759       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012122525 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.11       |\n",
      "|    explained_variance   | 0.904       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0396     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0132     |\n",
      "|    value_loss           | 0.304       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 866         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 61          |\n",
      "|    total_timesteps      | 52843       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012695575 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.12       |\n",
      "|    explained_variance   | 0.715       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.722       |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0147     |\n",
      "|    value_loss           | 0.624       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<utils.sb3.custom_ppo.MultiAgentPPO at 0x7f96ec07a650>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CustomActorCriticPolicy, \n",
    "model = MultiAgentPPO(env=env, policy=\"MlpPolicy\", verbose=1)\n",
    "model.learn(50_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, List, Optional, Tuple, Type, Union\n",
    "\n",
    "from gymnasium import spaces\n",
    "import torch as th\n",
    "from torch import nn\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "\n",
    "\n",
    "class CustomNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom network for policy and value function.\n",
    "    It receives as input the features extracted by the features extractor.\n",
    "\n",
    "    :param feature_dim: dimension of the features extracted with the features_extractor (e.g. features from a CNN)\n",
    "    :param last_layer_dim_pi: (int) number of units for the last layer of the policy network\n",
    "    :param last_layer_dim_vf: (int) number of units for the last layer of the value network\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim: int,\n",
    "        last_layer_dim_pi: int = 64,\n",
    "        last_layer_dim_vf: int = 64,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # IMPORTANT:\n",
    "        # Save output dimensions, used to create the distributions\n",
    "        self.latent_dim_pi = last_layer_dim_pi\n",
    "        self.latent_dim_vf = last_layer_dim_vf\n",
    "\n",
    "        # Policy network\n",
    "        self.policy_net = nn.Sequential(\n",
    "            nn.Linear(feature_dim, last_layer_dim_pi), nn.ReLU()\n",
    "        )\n",
    "        # Value network\n",
    "        self.value_net = nn.Sequential(\n",
    "            nn.Linear(feature_dim, last_layer_dim_vf), nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, features: th.Tensor) -> Tuple[th.Tensor, th.Tensor]:\n",
    "        \"\"\"\n",
    "        :return: (th.Tensor, th.Tensor) latent_policy, latent_value of the specified network.\n",
    "            If all layers are shared, then ``latent_policy == latent_value``\n",
    "        \"\"\"\n",
    "        return self.forward_actor(features), self.forward_critic(features)\n",
    "\n",
    "    def forward_actor(self, features: th.Tensor) -> th.Tensor:\n",
    "        return self.policy_net(features)\n",
    "\n",
    "    def forward_critic(self, features: th.Tensor) -> th.Tensor:\n",
    "        return self.value_net(features)\n",
    "\n",
    "\n",
    "class CustomActorCriticPolicy(ActorCriticPolicy):\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_space: spaces.Space,\n",
    "        action_space: spaces.Space,\n",
    "        lr_schedule: Callable[[float], float],\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # Disable orthogonal initialization\n",
    "        kwargs[\"ortho_init\"] = False\n",
    "        super().__init__(\n",
    "            observation_space,\n",
    "            action_space,\n",
    "            lr_schedule,\n",
    "            # Pass remaining arguments to base class\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "\n",
    "    def _build_mlp_extractor(self) -> None:\n",
    "        self.mlp_extractor = CustomNetwork(self.features_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1183 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 3    |\n",
      "|    total_timesteps | 4048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 976         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 8147        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012126818 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.21       |\n",
      "|    explained_variance   | 0.0163      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0875      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00909    |\n",
      "|    value_loss           | 0.512       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<utils.sb3.custom_ppo.MultiAgentPPO at 0x7f96ec07a6e0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CustomActorCriticPolicy, \n",
    "model = MultiAgentPPO(env=env, policy=\"MlpPolicy\", verbose=1)\n",
    "model.learn(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the network permutation equivariant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom network for policy and value function.\n",
    "    It receives as input the features extracted by the features extractor.\n",
    "\n",
    "    :param feature_dim: dimension of the features extracted with the features_extractor (e.g. features from a CNN)\n",
    "    :param last_layer_dim_pi: (int) number of units for the last layer of the policy network\n",
    "    :param last_layer_dim_vf: (int) number of units for the last layer of the value network\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim: int,\n",
    "        last_layer_dim_pi: int = 64,\n",
    "        last_layer_dim_vf: int = 64,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # IMPORTANT:\n",
    "        # Save output dimensions, used to create the distributions\n",
    "        self.latent_dim_pi = last_layer_dim_pi\n",
    "        self.latent_dim_vf = last_layer_dim_vf\n",
    "\n",
    "        # Policy network\n",
    "        self.policy_net = nn.Sequential(\n",
    "            nn.Linear(feature_dim, last_layer_dim_pi), nn.ReLU()\n",
    "        )\n",
    "        # Value network\n",
    "        self.value_net = nn.Sequential(\n",
    "            nn.Linear(feature_dim, last_layer_dim_vf), nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, features: th.Tensor) -> Tuple[th.Tensor, th.Tensor]:\n",
    "        \"\"\"\n",
    "        :return: (th.Tensor, th.Tensor) latent_policy, latent_value of the specified network.\n",
    "            If all layers are shared, then ``latent_policy == latent_value``\n",
    "        \"\"\"\n",
    "        return self.forward_actor(features), self.forward_critic(features)\n",
    "\n",
    "    def forward_actor(self, features: th.Tensor) -> th.Tensor:\n",
    "        return self.policy_net(features)\n",
    "\n",
    "    def forward_critic(self, features: th.Tensor) -> th.Tensor:\n",
    "        return self.value_net(features)\n",
    "\n",
    "\n",
    "class CustomActorCriticPolicy(ActorCriticPolicy):\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_space: spaces.Space,\n",
    "        action_space: spaces.Space,\n",
    "        lr_schedule: Callable[[float], float],\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # Disable orthogonal initialization\n",
    "        kwargs[\"ortho_init\"] = False\n",
    "        super().__init__(\n",
    "            observation_space,\n",
    "            action_space,\n",
    "            lr_schedule,\n",
    "            # Pass remaining arguments to base class\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "\n",
    "    def _build_mlp_extractor(self) -> None:\n",
    "        self.mlp_extractor = CustomNetwork(self.features_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch as th\n",
    "from typing import Tuple\n",
    "\n",
    "class PermEqNetwork(nn.Module):\n",
    "    def __init__(self, latent_dim_pi: int = 64, latent_dim_vf: int = 64):\n",
    "        super().__init__()\n",
    "        self.state_dim = 10\n",
    "        self.observation_dim = 6720\n",
    "\n",
    "        self.latent_dim_pi = latent_dim_pi\n",
    "        self.latent_dim_vf = latent_dim_vf\n",
    "\n",
    "        # Policy network for state\n",
    "        self.state_net = nn.Sequential(\n",
    "            nn.Linear(self.state_dim, latent_dim_pi),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Policy network for observation\n",
    "        self.observation_net = nn.Sequential(\n",
    "            nn.Linear(self.observation_dim, latent_dim_pi),\n",
    "            nn.Linear(latent_dim_pi, latent_dim_pi),\n",
    "            nn.Linear(latent_dim_pi, latent_dim_pi),\n",
    "            nn.Linear(latent_dim_pi, latent_dim_pi),\n",
    "            nn.Linear(latent_dim_pi, latent_dim_pi)\n",
    "        )\n",
    "\n",
    "        # Value network for state\n",
    "        self.value_state_net = nn.Sequential(\n",
    "            nn.Linear(self.state_dim, latent_dim_vf),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Value network for observation\n",
    "        self.value_observation_net = nn.Sequential(\n",
    "            nn.Linear(self.observation_dim, latent_dim_vf),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Attention layer for observation\n",
    "        self.attention = nn.MultiheadAttention(latent_dim_pi, num_heads=1)\n",
    "\n",
    "        # Final layer combining state and processed observation\n",
    "        self.final_layer = nn.Linear(latent_dim_pi + latent_dim_vf, latent_dim_pi)\n",
    "\n",
    "    def forward(self, features: th.Tensor) -> Tuple[th.Tensor, th.Tensor]:\n",
    "        state, observation = features[:, :10], features[:, 10:]\n",
    "\n",
    "        # Process state and observation separately through policy networks\n",
    "        latent_policy_state = self.state_net(state)\n",
    "        latent_policy_observation = self.observation_net(observation)\n",
    "\n",
    "        # Process state and observation through value networks\n",
    "        latent_value_state = self.value_state_net(state)\n",
    "        latent_value_observation = self.value_observation_net(observation)\n",
    "\n",
    "        # Attention mechanism on the observation\n",
    "        observation = observation.permute(1, 0, 2)  # Change shape for MultiheadAttention\n",
    "        output, _ = self.attention(latent_policy_observation.unsqueeze(0), observation, observation)\n",
    "        latent_policy_observation = th.max(output.squeeze(0), dim=0).values\n",
    "\n",
    "        # Concatenate policy and value outputs\n",
    "        latent_policy = th.cat((latent_policy_state, latent_policy_observation), dim=1)\n",
    "        latent_value = th.cat((latent_value_state, latent_value_observation), dim=1)\n",
    "\n",
    "        # Combine policy and value outputs\n",
    "        final_output = th.cat((latent_policy, latent_value), dim=1)\n",
    "        final_output = self.final_layer(final_output)\n",
    "\n",
    "        return final_output[:, :self.latent_dim_pi], final_output[:, self.latent_dim_pi:]\n",
    "\n",
    "\n",
    "class PermEqActorCriticPolicy(ActorCriticPolicy):\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_space: spaces.Space,\n",
    "        action_space: spaces.Space,\n",
    "        lr_schedule: Callable[[float], float],\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # Disable orthogonal initialization\n",
    "        kwargs[\"ortho_init\"] = False\n",
    "        super().__init__(\n",
    "            observation_space,\n",
    "            action_space,\n",
    "            lr_schedule,\n",
    "            # Pass remaining arguments to base class\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "\n",
    "    def _build_mlp_extractor(self) -> None:\n",
    "        self.mlp_extractor = PermEqNetwork(self.features_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/emerge/daphne/nocturne_lab/experiments/rl/perm_eq_net.ipynb Cell 15\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Beng-dc4971-linux-01.engineering.nyu.edu/home/emerge/daphne/nocturne_lab/experiments/rl/perm_eq_net.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#CustomActorCriticPolicy, \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Beng-dc4971-linux-01.engineering.nyu.edu/home/emerge/daphne/nocturne_lab/experiments/rl/perm_eq_net.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m MultiAgentPPO(env\u001b[39m=\u001b[39menv, policy\u001b[39m=\u001b[39mPermEqActorCriticPolicy, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Beng-dc4971-linux-01.engineering.nyu.edu/home/emerge/daphne/nocturne_lab/experiments/rl/perm_eq_net.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m model\u001b[39m.\u001b[39;49mlearn(\u001b[39m5000\u001b[39;49m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/envs/nocturne_lab/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:308\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    300\u001b[0m     \u001b[39mself\u001b[39m: SelfPPO,\n\u001b[1;32m    301\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    307\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 308\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    309\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    310\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    311\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    312\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    313\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    314\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m    315\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/envs/nocturne_lab/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:259\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 259\u001b[0m     continue_training \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, callback, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrollout_buffer, n_rollout_steps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_steps)\n\u001b[1;32m    261\u001b[0m     \u001b[39mif\u001b[39;00m continue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/daphne/nocturne_lab/utils/sb3/custom_ppo.py:78\u001b[0m, in \u001b[0;36mMultiAgentPPO.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m     75\u001b[0m     obs_tensor_alive \u001b[39m=\u001b[39m obs_tensor[alive_agent_idx, :]\n\u001b[1;32m     77\u001b[0m     \u001b[39m# Predict actions, vals and log_probs given obs\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m     actions_tmp, values_tmp, log_prob_tmp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy(obs_tensor_alive)\n\u001b[1;32m     79\u001b[0m     actions[alive_agent_idx], values[alive_agent_idx], log_probs[alive_agent_idx] \u001b[39m=\u001b[39m actions_tmp\u001b[39m.\u001b[39mfloat(), values_tmp\u001b[39m.\u001b[39mfloat(), log_prob_tmp\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m     81\u001b[0m actions \u001b[39m=\u001b[39m actions\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/envs/nocturne_lab/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/envs/nocturne_lab/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/envs/nocturne_lab/lib/python3.10/site-packages/stable_baselines3/common/policies.py:619\u001b[0m, in \u001b[0;36mActorCriticPolicy.forward\u001b[0;34m(self, obs, deterministic)\u001b[0m\n\u001b[1;32m    617\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mextract_features(obs)\n\u001b[1;32m    618\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshare_features_extractor:\n\u001b[0;32m--> 619\u001b[0m     latent_pi, latent_vf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp_extractor(features)\n\u001b[1;32m    620\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    621\u001b[0m     pi_features, vf_features \u001b[39m=\u001b[39m features\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/envs/nocturne_lab/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/envs/nocturne_lab/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/home/emerge/daphne/nocturne_lab/experiments/rl/perm_eq_net.ipynb Cell 15\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Beng-dc4971-linux-01.engineering.nyu.edu/home/emerge/daphne/nocturne_lab/experiments/rl/perm_eq_net.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m latent_value_observation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue_observation_net(observation)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Beng-dc4971-linux-01.engineering.nyu.edu/home/emerge/daphne/nocturne_lab/experiments/rl/perm_eq_net.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39m# Attention mechanism on the observation\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Beng-dc4971-linux-01.engineering.nyu.edu/home/emerge/daphne/nocturne_lab/experiments/rl/perm_eq_net.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m observation \u001b[39m=\u001b[39m observation\u001b[39m.\u001b[39;49mpermute(\u001b[39m1\u001b[39;49m, \u001b[39m0\u001b[39;49m, \u001b[39m2\u001b[39;49m)  \u001b[39m# Change shape for MultiheadAttention\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Beng-dc4971-linux-01.engineering.nyu.edu/home/emerge/daphne/nocturne_lab/experiments/rl/perm_eq_net.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m output, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention(latent_policy_observation\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m), observation, observation)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Beng-dc4971-linux-01.engineering.nyu.edu/home/emerge/daphne/nocturne_lab/experiments/rl/perm_eq_net.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m latent_policy_observation \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39mmax(output\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m), dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mvalues\n",
      "\u001b[0;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 3"
     ]
    }
   ],
   "source": [
    "#CustomActorCriticPolicy, \n",
    "model = MultiAgentPPO(env=env, policy=PermEqActorCriticPolicy, verbose=1)\n",
    "model.learn(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nocturne_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
